{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# &#x1F4D1; &nbsp;  $\\mathfrak {\\color{#348ABD} {P5: \\ Build \\ a \\ Digit \\ Recognition \\ Program. \\ Step \\ 3}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## $\\mathfrak {\\color{#348ABD} {Code \\ Library \\ and \\ Links}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-dimensional image processing https://docs.scipy.org/doc/scipy/reference/ndimage.html\n",
    "\n",
    "Keras: Deep Learning library for Theano and TensorFlow https://keras.io/\n",
    " \n",
    "Deep MNIST for Experts https://www.tensorflow.org/get_started/mnist/pros\n",
    "\n",
    "Tensorflow Deep MNIST Advanced Tutorial http://docs.seldon.io/tensorflow-deep-mnist-example.html\n",
    "\n",
    "Handwritten Digit Recognition using Convolutional Neural Networks in Python with Keras\n",
    "\n",
    "http://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>code_show = true; \n",
       "function code_display() {\n",
       "    if (code_show) {\n",
       "        $('div.input').each(function(id) {\n",
       "            if (id == 0 || $(this).html().indexOf('hide_code') > -1) {$(this).hide();}\n",
       "        });\n",
       "        $('div.output_prompt').css('opacity', 0);\n",
       "    } else {\n",
       "        $('div.input').each(function(id) {$(this).show();});\n",
       "        $('div.output_prompt').css('opacity', 1);\n",
       "    }\n",
       "    code_show = !code_show;\n",
       "} \n",
       "$(document).ready(code_display);</script>\n",
       "<form action=\"javascript: code_display()\"><input style=\"color: #348ABD; background: ghostwhite; opacity: 0.9; \" type=\"submit\" value=\"Click to display or hide code\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "hide_code = ''\n",
    "HTML('''<script>code_show = true; \n",
    "function code_display() {\n",
    "    if (code_show) {\n",
    "        $('div.input').each(function(id) {\n",
    "            if (id == 0 || $(this).html().indexOf('hide_code') > -1) {$(this).hide();}\n",
    "        });\n",
    "        $('div.output_prompt').css('opacity', 0);\n",
    "    } else {\n",
    "        $('div.input').each(function(id) {$(this).show();});\n",
    "        $('div.output_prompt').css('opacity', 1);\n",
    "    }\n",
    "    code_show = !code_show;\n",
    "} \n",
    "$(document).ready(code_display);</script>\n",
    "<form action=\"javascript: code_display()\"><input style=\"color: #348ABD; background: ghostwhite; opacity: 0.9; \" \\\n",
    "type=\"submit\" value=\"Click to display or hide code\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hide_code\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import scipy.ndimage\n",
    "from scipy.special import expit\n",
    "from time import time\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import tarfile\n",
    "\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "from IPython.display import display, Image, IFrame\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import offsetbox\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hide_code\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.examples.tutorials.mnist as mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hide_code\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier, BernoulliRBM\n",
    "from sklearn import linear_model, datasets, metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import manifold, decomposition, ensemble\n",
    "from sklearn import discriminant_analysis, random_projection\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "hide_code\n",
    "\n",
    "import keras as ks\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.layers import Activation, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Conv2D, MaxPooling2D\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## $\\mathfrak {\\color{#348ABD} {Experimental \\ Datasets}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset #6. Newly-Captured Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image examples: \n",
    "<div style=\"width: 960px; height: 50px; overflow:auto;\">\n",
    "    <img src=\"00.png\" width=\"30\" height=\"30\"/>\n",
    "\t<img src=\"01.png\" width=\"30\" height=\"30\"/>\n",
    "\t<img src=\"02.png\" width=\"30\" height=\"30\"/>\n",
    "\t<img src=\"03.png\" width=\"30\" height=\"30\"/>\n",
    "\t<img src=\"04.png\" width=\"30\" height=\"30\"/>\n",
    "\t<img src=\"05.png\" width=\"30\" height=\"30\"/>\n",
    "\t<img src=\"06.png\" width=\"30\" height=\"30\"/>\n",
    "\t<img src=\"07.png\" width=\"30\" height=\"30\"/>\n",
    "\t<img src=\"08.png\" width=\"30\" height=\"30\"/>\n",
    "\t<img src=\"09.png\" width=\"30\" height=\"30\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hide_code\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hide_code\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hide_code\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hide_code\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "## $\\mathfrak {\\color{#348ABD} {Step \\ 3: \\ Test \\ a \\ Model \\ on \\ Newly-Captured \\ Images}}$\n",
    "\n",
    "Take several pictures of numbers that you find around you (at least five), and run them through your classifier on your computer to produce example results. Alternatively (optionally), you can try using OpenCV / SimpleCV / Pygame to capture live images from a webcam and run those through your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hide_code\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hide_code\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hide_code\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "_Choose five candidate images of numbers you took from around you and provide them in the report. Are there any particular qualities of the image(s) that might make classification difficult?_\n",
    "\n",
    "### Answer 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "_Is your model able to perform equally well on captured pictures or a live camera stream when compared to testing on the realistic dataset?_\n",
    "\n",
    "### Answer 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Question 9\n",
    "_If necessary, provide documentation for how an interface was built for your model to load and classify newly-acquired images._\n",
    "\n",
    "### Answer 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## $\\mathfrak {\\color{#348ABD} {Step \\ 4: \\ Explore \\ an \\ Improvement \\ for \\ a \\ Model}}$\n",
    "\n",
    "There are many things you can do once you have the basic classifier in place. One example would be to also localize where the numbers are on the image. The SVHN dataset provides bounding boxes that you can tune to train a localizer. Train a regression loss to the coordinates of the bounding box, and then test it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hide_code\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hide_code\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hide_code\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "_How well does your model localize numbers on the testing set from the realistic dataset? Do your classification results change at all with localization included?_\n",
    "### Answer 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question 11\n",
    "_Test the localization function on the images you captured in **Step 3**. Does the model accurately calculate a bounding box for the numbers in the images you found? If you did not use a graphical interface, you may need to investigate the bounding boxes by hand._ Provide an example of the localization created on a captured image.\n",
    "\n",
    "### Answer 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "## $\\mathfrak {\\color{#348ABD} {Optional \\ Step \\ 5: \\ Build \\ an \\ Application \\ or \\ Program \\ for \\ a \\ Model}}$\n",
    "Take your project one step further. If you're interested, look to build an Android application or even a more robust Python program that can interface with input images and display the classified numbers and even the bounding boxes. You can for example try to build an augmented reality app by overlaying your answer on the image like the [Word Lens](https://en.wikipedia.org/wiki/Word_Lens) app does.\n",
    "\n",
    "Loading a TensorFlow model into a camera app on Android is demonstrated in the [TensorFlow Android demo app](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android), which you can simply modify.\n",
    "\n",
    "If you decide to explore this optional route, be sure to document your interface and implementation, along with significant results you find. You can see the additional rubric items that you could be evaluated on by [following this link](https://review.udacity.com/#!/rubrics/413/view)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hide_code\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hide_code\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hide_code\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation\n",
    "Provide additional documentation sufficient for detailing the implementation of the Android application or Python program for visualizing the classification of numbers in images. It should be clear how the program or application works. Demonstrations should be provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hide_code\n",
    "\n",
    "\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
